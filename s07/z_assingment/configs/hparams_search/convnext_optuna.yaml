# @package _global_

# ConvNeXt Hyperparameter Optimization with Optuna on Fashion MNIST
# 
# Run locally:
#   python src/train.py -m hparams_search=convnext_optuna trainer.max_epochs=2
#
# GitHub Actions will run this and post results as a comment

defaults:
  - override /hydra/sweeper: optuna
  - override /data: fashionmnist

# Override model to use ConvNeXt (small and efficient)
model:
  base_model: "convnext_atto"  # Smallest ConvNeXt variant
  pretrained: True
  num_classes: 10  # Fashion MNIST has 10 classes

# Experiment tracking
experiment_name: "convnext-fashionmnist-hparam-search"

# Training settings (optimized for CPU in CI)
trainer:
  max_epochs: 2
  log_every_n_steps: 10
  accelerator: auto
  # Limit batches for faster CI runs (remove for full training)
  limit_train_batches: 100  # ~6400 samples
  limit_val_batches: 20     # ~1280 samples

# Data settings (smaller batch for CPU)
data:
  batch_size: 64
  num_workers: 2
  image_size: 32  # Much faster than 224!

# Choose metric to optimize
optimized_metric: "val/acc"

# Optuna configuration
hydra:
  mode: "MULTIRUN"

  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper

    # Persist results to SQLite for tracking
    storage: null
    study_name: "convnext_fashionmnist_hparam_search"

    n_jobs: 1
    direction: maximize
    n_trials: 10  # Run 10 experiments

    # TPE Sampler (Bayesian optimization)
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 42
      n_startup_trials: 3

    # Hyperparameter search space for ConvNeXt
    params:
      # Learning rate (log scale)
      model.lr: tag(log, interval(1e-5, 1e-2))
      
      # Weight decay
      model.weight_decay: tag(log, interval(1e-6, 1e-3))
      
      # Batch size options
      data.batch_size: choice(32, 64, 128)
      
      # Optimizer patience for LR scheduler
      model.patience: choice(3, 5, 10)
      
      # Model variants (all small ConvNeXt)
      model.base_model: choice(convnext_atto, convnext_femto, convnext_pico)

